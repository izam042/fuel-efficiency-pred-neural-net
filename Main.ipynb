{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFIo3rurVpzLth9+7hiwcg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "IIWt_NQQIGKk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/' 'machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "\n",
        "coloumn_names =  ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "df = pd.read_csv(url, names=coloumn_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "import torch\n",
        "\n",
        "df_train,df_test = sklearn.model_selection.train_test_split(df,train_size=0.8 , random_state=42)\n",
        "\n",
        "train_stats = df_train.describe().transpose()\n",
        "\n",
        "numeric_coloumn_names=['Cylinders','Displacement','Horsepower','Weight','Acceleration']\n",
        "\n",
        "df_train_norm, df_test_norm = df_train.copy(), df_test.copy()\n",
        "\n",
        "\n",
        "for col_name in numeric_coloumn_names:\n",
        "    mean = train_stats.loc[col_name,'mean']\n",
        "    std = train_stats.loc[col_name,'std']\n",
        "    df_train_norm.loc[:, col_name] = \\\n",
        "    (df_train_norm.loc[:, col_name] - mean)/std\n",
        "    df_test_norm.loc[:, col_name] = \\\n",
        "    (df_test_norm.loc[:, col_name] - mean)/std\n",
        "\n",
        "\n",
        "boundaries = torch.tensor([73, 76, 79])\n",
        "v = torch.tensor(df_train_norm['Model Year'].values)\n",
        "df_train_norm['Model Year Bucketed'] = torch.bucketize(\n",
        "    v, boundaries, right=True\n",
        ")\n",
        "v = torch.tensor(df_test_norm['Model Year'].values)\n",
        "df_test_norm['Model Year Bucketed'] = torch.bucketize(\n",
        "    v, boundaries, right=True\n",
        ")\n",
        "\n",
        "numeric_coloumn_names.append('Model Year Bucketed')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z6J3TKRCJO9g",
        "outputId": "371186f6-592c-4e66-b354-b5fc82e0e41b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1433301458.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30437417 -0.87178774  0.30437417  1.48053607 -0.87178774  1.48053607\n",
            " -0.87178774  0.30437417  0.30437417  1.48053607  0.30437417 -0.87178774\n",
            "  0.30437417 -0.87178774 -0.87178774  0.30437417  0.30437417 -0.87178774\n",
            "  1.48053607 -0.87178774  0.30437417  1.48053607  0.30437417  1.48053607\n",
            " -0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            "  1.48053607 -0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            " -0.87178774  1.48053607  0.30437417 -0.87178774  0.30437417 -0.87178774\n",
            "  0.30437417  0.30437417 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            " -0.87178774  0.30437417 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            " -0.87178774  1.48053607 -0.87178774 -0.87178774 -0.87178774  0.30437417\n",
            "  1.48053607  1.48053607  0.30437417 -0.87178774 -0.87178774 -0.87178774\n",
            " -0.87178774 -0.87178774  1.48053607  0.30437417 -0.87178774  1.48053607\n",
            " -0.87178774  1.48053607 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            "  1.48053607 -0.87178774 -0.87178774  1.48053607 -0.87178774 -0.87178774\n",
            "  1.48053607  1.48053607  1.48053607  1.48053607  0.30437417 -0.87178774\n",
            " -0.87178774  0.30437417  0.30437417  1.48053607 -0.87178774  1.48053607\n",
            " -0.87178774  0.30437417  1.48053607 -0.87178774 -0.87178774  0.30437417\n",
            " -0.87178774 -0.87178774  0.30437417 -0.87178774  1.48053607  1.48053607\n",
            "  0.30437417  0.30437417 -1.45986869 -0.87178774 -0.87178774  1.48053607\n",
            " -0.87178774 -0.87178774 -0.87178774  1.48053607 -0.87178774 -0.87178774\n",
            "  1.48053607  1.48053607 -0.87178774  1.48053607  1.48053607  0.30437417\n",
            " -0.87178774 -0.87178774 -0.28370679 -0.87178774  0.30437417 -0.87178774\n",
            " -0.87178774  0.30437417 -0.87178774  1.48053607 -0.87178774  1.48053607\n",
            "  1.48053607  0.30437417  0.30437417 -0.87178774  1.48053607 -0.87178774\n",
            "  1.48053607  1.48053607 -0.87178774  1.48053607  1.48053607 -0.87178774\n",
            "  0.30437417 -0.87178774  0.30437417 -0.87178774  0.30437417 -0.87178774\n",
            "  1.48053607 -0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            " -0.87178774  1.48053607  0.30437417 -0.87178774  1.48053607  0.30437417\n",
            " -0.87178774  1.48053607  1.48053607  1.48053607 -0.87178774  1.48053607\n",
            " -1.45986869  0.30437417  1.48053607  0.30437417  1.48053607 -0.87178774\n",
            " -0.87178774 -0.87178774 -0.87178774  1.48053607  1.48053607 -0.87178774\n",
            " -0.87178774 -0.87178774  1.48053607 -0.87178774  1.48053607  0.30437417\n",
            "  1.48053607  0.30437417 -0.28370679  1.48053607  0.30437417 -0.87178774\n",
            "  0.30437417  0.30437417 -0.87178774  1.48053607 -0.87178774  0.30437417\n",
            " -0.87178774  1.48053607  0.30437417  0.30437417 -0.87178774  0.30437417\n",
            " -0.87178774  1.48053607 -0.87178774  1.48053607  1.48053607  1.48053607\n",
            "  0.30437417 -0.87178774  1.48053607  1.48053607  1.48053607 -0.87178774\n",
            "  1.48053607  1.48053607 -0.87178774 -0.87178774  1.48053607  1.48053607\n",
            "  0.30437417 -0.87178774 -0.87178774  0.30437417 -0.87178774 -0.87178774\n",
            " -0.87178774  1.48053607 -0.28370679  0.30437417  0.30437417 -0.87178774\n",
            "  0.30437417 -0.87178774 -0.87178774 -0.87178774  0.30437417  1.48053607\n",
            " -0.87178774  0.30437417 -0.87178774  0.30437417  1.48053607 -0.87178774\n",
            "  1.48053607 -0.87178774 -0.87178774 -0.87178774  0.30437417  1.48053607\n",
            "  1.48053607 -0.87178774 -0.87178774 -1.45986869  1.48053607 -0.87178774\n",
            "  1.48053607  0.30437417 -0.87178774 -0.87178774 -0.87178774  1.48053607\n",
            " -0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            "  0.30437417  0.30437417 -0.87178774  1.48053607 -0.87178774 -0.87178774\n",
            " -0.87178774 -0.87178774 -0.87178774  0.30437417 -0.87178774 -0.87178774\n",
            "  0.30437417 -0.87178774  0.30437417 -0.87178774 -0.87178774  0.30437417\n",
            " -0.87178774 -0.87178774 -0.87178774  0.30437417  1.48053607  0.30437417\n",
            " -0.87178774  1.48053607  0.30437417 -0.87178774 -0.87178774 -0.87178774\n",
            " -0.87178774  1.48053607  1.48053607  0.30437417 -0.87178774 -0.87178774\n",
            "  1.48053607]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df_train_norm.loc[:, col_name] = \\\n",
            "/tmp/ipython-input-1433301458.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774 -0.87178774\n",
            "  1.48053607 -0.87178774  0.30437417 -0.87178774  1.48053607  0.30437417\n",
            "  1.48053607 -0.87178774  0.30437417 -0.87178774  0.30437417 -0.87178774\n",
            " -0.87178774 -0.87178774  0.30437417 -0.87178774 -0.87178774  1.48053607\n",
            " -0.87178774 -0.87178774 -0.87178774  0.30437417 -0.87178774 -0.87178774\n",
            "  1.48053607  0.30437417  1.48053607 -0.87178774  1.48053607 -0.87178774\n",
            "  1.48053607 -0.87178774  1.48053607  1.48053607  1.48053607 -0.87178774\n",
            " -0.87178774 -0.87178774  1.48053607  1.48053607  0.30437417 -0.87178774\n",
            " -0.87178774 -0.87178774  1.48053607 -0.87178774 -0.87178774 -0.87178774\n",
            "  0.30437417  0.30437417  1.48053607  0.30437417 -0.87178774 -0.87178774\n",
            "  1.48053607  0.30437417  0.30437417 -0.87178774 -0.87178774 -0.87178774\n",
            " -0.87178774 -0.87178774  1.48053607  1.48053607 -0.87178774  1.48053607\n",
            "  0.30437417 -1.45986869  0.30437417 -0.87178774  1.48053607  1.48053607\n",
            " -0.87178774]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df_test_norm.loc[:, col_name] = \\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import one_hot\n",
        "total_origin = len(set(df_train_norm['Origin']))\n",
        "origin_encoded = one_hot(\n",
        "    torch.from_numpy(df_train_norm['Origin'].values) % total_origin\n",
        ")\n",
        "\n",
        "x_train_numeric = torch.tensor(\n",
        "    df_train_norm[numeric_coloumn_names].values)\n",
        "x_train = torch.cat([x_train_numeric, origin_encoded], 1).float()\n",
        "#testset\n",
        "\n",
        "origin_encoded = one_hot(torch.from_numpy(\n",
        "    df_test_norm['Origin'].values) % total_origin)\n",
        "x_test_numeric = torch.tensor(\n",
        "    df_test_norm[numeric_coloumn_names].values)\n",
        "x_test = torch.cat([x_test_numeric, origin_encoded], 1).float()\n",
        "\n",
        "y_train = torch.tensor(df_train_norm['MPG'].values).float()\n",
        "y_test = torch.tensor(df_test_norm['MPG'].values).float()\n"
      ],
      "metadata": {
        "id": "BHezXct8ZeJw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "train_ds = TensorDataset(x_train,y_train)\n",
        "batch_size= 8\n",
        "torch.manual_seed(42)\n",
        "train_dl = DataLoader(train_ds,batch_size,shuffle=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wdMNQ_bTbK8S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "hidden_units =[8,4]\n",
        "input_size = x_train.shape[1]\n",
        "all_layers=[]\n",
        "\n",
        "for input in hidden_units:\n",
        "  layer = torch.nn.Linear(input_size,input)\n",
        "  all_layers.append(layer)\n",
        "  all_layers.append(torch.nn.ReLU())\n",
        "  input_size = input\n",
        "all_layers.append(torch.nn.Linear(input_size,1))\n",
        "model = nn.Sequential(*all_layers)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer =torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "num_epochs=200\n",
        "log_epochs=20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   loss_hist_train = 0\n",
        "   for x_batch,y_batch in train_dl:\n",
        "    y_pred = model(x_batch)[:,0]\n",
        "    loss = loss_fn(y_pred,y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    loss_hist_train += loss.item()\n",
        "\n",
        "    if epoch % log_epochs==0:\n",
        "      print(f'Epoch {epoch}  Loss '\n",
        "f'{loss_hist_train/len(train_dl):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZgLdb45cuUj",
        "outputId": "253703f2-4280-4e75-f1fd-c3c46699a0b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0  Loss 21.9548\n",
            "Epoch 0  Loss 36.6743\n",
            "Epoch 0  Loss 53.7708\n",
            "Epoch 0  Loss 66.7633\n",
            "Epoch 0  Loss 80.7989\n",
            "Epoch 0  Loss 98.9014\n",
            "Epoch 0  Loss 111.3136\n",
            "Epoch 0  Loss 135.6047\n",
            "Epoch 0  Loss 148.2592\n",
            "Epoch 0  Loss 167.4387\n",
            "Epoch 0  Loss 180.4427\n",
            "Epoch 0  Loss 199.4265\n",
            "Epoch 0  Loss 211.2907\n",
            "Epoch 0  Loss 224.7955\n",
            "Epoch 0  Loss 242.4589\n",
            "Epoch 0  Loss 256.7277\n",
            "Epoch 0  Loss 270.4195\n",
            "Epoch 0  Loss 296.0091\n",
            "Epoch 0  Loss 312.3442\n",
            "Epoch 0  Loss 325.8993\n",
            "Epoch 0  Loss 345.8016\n",
            "Epoch 0  Loss 359.3058\n",
            "Epoch 0  Loss 377.1546\n",
            "Epoch 0  Loss 390.9140\n",
            "Epoch 0  Loss 410.7626\n",
            "Epoch 0  Loss 420.6164\n",
            "Epoch 0  Loss 437.8029\n",
            "Epoch 0  Loss 460.5438\n",
            "Epoch 0  Loss 473.5818\n",
            "Epoch 0  Loss 489.6910\n",
            "Epoch 0  Loss 500.4035\n",
            "Epoch 0  Loss 520.4336\n",
            "Epoch 0  Loss 534.4771\n",
            "Epoch 0  Loss 553.4636\n",
            "Epoch 0  Loss 571.1034\n",
            "Epoch 0  Loss 587.1389\n",
            "Epoch 0  Loss 600.4144\n",
            "Epoch 0  Loss 609.8326\n",
            "Epoch 0  Loss 624.6244\n",
            "Epoch 0  Loss 630.8022\n",
            "Epoch 20  Loss 0.5570\n",
            "Epoch 20  Loss 0.7238\n",
            "Epoch 20  Loss 0.8120\n",
            "Epoch 20  Loss 1.1424\n",
            "Epoch 20  Loss 1.5616\n",
            "Epoch 20  Loss 1.6272\n",
            "Epoch 20  Loss 2.3570\n",
            "Epoch 20  Loss 2.8131\n",
            "Epoch 20  Loss 3.0229\n",
            "Epoch 20  Loss 3.3112\n",
            "Epoch 20  Loss 3.5611\n",
            "Epoch 20  Loss 3.6996\n",
            "Epoch 20  Loss 4.2748\n",
            "Epoch 20  Loss 4.3222\n",
            "Epoch 20  Loss 4.3855\n",
            "Epoch 20  Loss 4.6121\n",
            "Epoch 20  Loss 5.2999\n",
            "Epoch 20  Loss 5.7064\n",
            "Epoch 20  Loss 5.8699\n",
            "Epoch 20  Loss 6.2501\n",
            "Epoch 20  Loss 6.3474\n",
            "Epoch 20  Loss 6.6321\n",
            "Epoch 20  Loss 7.1008\n",
            "Epoch 20  Loss 7.4492\n",
            "Epoch 20  Loss 7.5998\n",
            "Epoch 20  Loss 7.8702\n",
            "Epoch 20  Loss 8.0826\n",
            "Epoch 20  Loss 8.4106\n",
            "Epoch 20  Loss 8.5545\n",
            "Epoch 20  Loss 8.8180\n",
            "Epoch 20  Loss 9.2546\n",
            "Epoch 20  Loss 9.4700\n",
            "Epoch 20  Loss 9.6950\n",
            "Epoch 20  Loss 9.9819\n",
            "Epoch 20  Loss 10.2258\n",
            "Epoch 20  Loss 10.3943\n",
            "Epoch 20  Loss 10.7908\n",
            "Epoch 20  Loss 11.3332\n",
            "Epoch 20  Loss 11.5111\n",
            "Epoch 20  Loss 11.5381\n",
            "Epoch 40  Loss 0.1585\n",
            "Epoch 40  Loss 0.3921\n",
            "Epoch 40  Loss 0.6952\n",
            "Epoch 40  Loss 1.1950\n",
            "Epoch 40  Loss 1.4606\n",
            "Epoch 40  Loss 1.6472\n",
            "Epoch 40  Loss 1.7443\n",
            "Epoch 40  Loss 1.8120\n",
            "Epoch 40  Loss 1.8997\n",
            "Epoch 40  Loss 2.0392\n",
            "Epoch 40  Loss 2.2044\n",
            "Epoch 40  Loss 2.3152\n",
            "Epoch 40  Loss 2.4312\n",
            "Epoch 40  Loss 2.6015\n",
            "Epoch 40  Loss 3.0146\n",
            "Epoch 40  Loss 3.1579\n",
            "Epoch 40  Loss 3.7624\n",
            "Epoch 40  Loss 4.0362\n",
            "Epoch 40  Loss 4.3374\n",
            "Epoch 40  Loss 4.5087\n",
            "Epoch 40  Loss 4.6690\n",
            "Epoch 40  Loss 4.8547\n",
            "Epoch 40  Loss 5.2947\n",
            "Epoch 40  Loss 5.5796\n",
            "Epoch 40  Loss 5.6395\n",
            "Epoch 40  Loss 5.7969\n",
            "Epoch 40  Loss 6.1069\n",
            "Epoch 40  Loss 6.6612\n",
            "Epoch 40  Loss 6.8709\n",
            "Epoch 40  Loss 7.0392\n",
            "Epoch 40  Loss 7.1272\n",
            "Epoch 40  Loss 7.1823\n",
            "Epoch 40  Loss 7.3500\n",
            "Epoch 40  Loss 7.8334\n",
            "Epoch 40  Loss 8.1541\n",
            "Epoch 40  Loss 8.5332\n",
            "Epoch 40  Loss 8.6571\n",
            "Epoch 40  Loss 8.8028\n",
            "Epoch 40  Loss 9.0392\n",
            "Epoch 40  Loss 9.0420\n",
            "Epoch 60  Loss 0.7346\n",
            "Epoch 60  Loss 0.9528\n",
            "Epoch 60  Loss 1.5624\n",
            "Epoch 60  Loss 1.6803\n",
            "Epoch 60  Loss 1.7233\n",
            "Epoch 60  Loss 2.2477\n",
            "Epoch 60  Loss 2.3145\n",
            "Epoch 60  Loss 2.4311\n",
            "Epoch 60  Loss 2.7148\n",
            "Epoch 60  Loss 2.9058\n",
            "Epoch 60  Loss 2.9410\n",
            "Epoch 60  Loss 2.9888\n",
            "Epoch 60  Loss 3.1511\n",
            "Epoch 60  Loss 3.2487\n",
            "Epoch 60  Loss 3.3384\n",
            "Epoch 60  Loss 3.4917\n",
            "Epoch 60  Loss 3.8729\n",
            "Epoch 60  Loss 4.1987\n",
            "Epoch 60  Loss 4.4576\n",
            "Epoch 60  Loss 4.6001\n",
            "Epoch 60  Loss 4.7833\n",
            "Epoch 60  Loss 4.9992\n",
            "Epoch 60  Loss 5.2600\n",
            "Epoch 60  Loss 5.5205\n",
            "Epoch 60  Loss 5.6197\n",
            "Epoch 60  Loss 5.7067\n",
            "Epoch 60  Loss 5.8717\n",
            "Epoch 60  Loss 5.9561\n",
            "Epoch 60  Loss 6.1306\n",
            "Epoch 60  Loss 6.2241\n",
            "Epoch 60  Loss 6.4485\n",
            "Epoch 60  Loss 6.7114\n",
            "Epoch 60  Loss 7.2694\n",
            "Epoch 60  Loss 7.4070\n",
            "Epoch 60  Loss 7.5881\n",
            "Epoch 60  Loss 7.7671\n",
            "Epoch 60  Loss 7.8611\n",
            "Epoch 60  Loss 7.9732\n",
            "Epoch 60  Loss 8.2684\n",
            "Epoch 60  Loss 8.3045\n",
            "Epoch 80  Loss 0.2201\n",
            "Epoch 80  Loss 0.3103\n",
            "Epoch 80  Loss 0.6107\n",
            "Epoch 80  Loss 0.9607\n",
            "Epoch 80  Loss 1.0674\n",
            "Epoch 80  Loss 1.1538\n",
            "Epoch 80  Loss 1.5819\n",
            "Epoch 80  Loss 1.8080\n",
            "Epoch 80  Loss 1.9637\n",
            "Epoch 80  Loss 2.1168\n",
            "Epoch 80  Loss 2.3115\n",
            "Epoch 80  Loss 2.3486\n",
            "Epoch 80  Loss 2.4468\n",
            "Epoch 80  Loss 2.6654\n",
            "Epoch 80  Loss 3.0960\n",
            "Epoch 80  Loss 3.2553\n",
            "Epoch 80  Loss 3.3962\n",
            "Epoch 80  Loss 3.4827\n",
            "Epoch 80  Loss 3.6266\n",
            "Epoch 80  Loss 3.7899\n",
            "Epoch 80  Loss 3.8844\n",
            "Epoch 80  Loss 4.0309\n",
            "Epoch 80  Loss 4.0643\n",
            "Epoch 80  Loss 4.2144\n",
            "Epoch 80  Loss 4.3174\n",
            "Epoch 80  Loss 4.4455\n",
            "Epoch 80  Loss 4.5387\n",
            "Epoch 80  Loss 4.6728\n",
            "Epoch 80  Loss 5.0131\n",
            "Epoch 80  Loss 5.5157\n",
            "Epoch 80  Loss 5.7859\n",
            "Epoch 80  Loss 5.9283\n",
            "Epoch 80  Loss 6.0171\n",
            "Epoch 80  Loss 6.1353\n",
            "Epoch 80  Loss 6.3081\n",
            "Epoch 80  Loss 6.3284\n",
            "Epoch 80  Loss 6.5687\n",
            "Epoch 80  Loss 6.9895\n",
            "Epoch 80  Loss 7.8396\n",
            "Epoch 80  Loss 8.9963\n",
            "Epoch 100  Loss 0.0923\n",
            "Epoch 100  Loss 0.2365\n",
            "Epoch 100  Loss 0.7524\n",
            "Epoch 100  Loss 0.8200\n",
            "Epoch 100  Loss 0.9278\n",
            "Epoch 100  Loss 1.2580\n",
            "Epoch 100  Loss 1.2746\n",
            "Epoch 100  Loss 1.4283\n",
            "Epoch 100  Loss 1.7352\n",
            "Epoch 100  Loss 1.9763\n",
            "Epoch 100  Loss 2.0761\n",
            "Epoch 100  Loss 2.3631\n",
            "Epoch 100  Loss 2.8727\n",
            "Epoch 100  Loss 3.0997\n",
            "Epoch 100  Loss 3.3169\n",
            "Epoch 100  Loss 3.4246\n",
            "Epoch 100  Loss 3.5383\n",
            "Epoch 100  Loss 3.8408\n",
            "Epoch 100  Loss 4.0529\n",
            "Epoch 100  Loss 4.0820\n",
            "Epoch 100  Loss 4.2911\n",
            "Epoch 100  Loss 4.4139\n",
            "Epoch 100  Loss 4.4815\n",
            "Epoch 100  Loss 4.7284\n",
            "Epoch 100  Loss 4.8142\n",
            "Epoch 100  Loss 5.1539\n",
            "Epoch 100  Loss 5.4425\n",
            "Epoch 100  Loss 5.8366\n",
            "Epoch 100  Loss 5.8932\n",
            "Epoch 100  Loss 6.1356\n",
            "Epoch 100  Loss 6.4449\n",
            "Epoch 100  Loss 6.7310\n",
            "Epoch 100  Loss 6.8994\n",
            "Epoch 100  Loss 7.0556\n",
            "Epoch 100  Loss 7.2279\n",
            "Epoch 100  Loss 7.3400\n",
            "Epoch 100  Loss 7.4845\n",
            "Epoch 100  Loss 7.6844\n",
            "Epoch 100  Loss 7.8337\n",
            "Epoch 100  Loss 7.8578\n",
            "Epoch 120  Loss 0.1386\n",
            "Epoch 120  Loss 0.6811\n",
            "Epoch 120  Loss 0.8464\n",
            "Epoch 120  Loss 1.0650\n",
            "Epoch 120  Loss 1.1136\n",
            "Epoch 120  Loss 1.3677\n",
            "Epoch 120  Loss 1.4920\n",
            "Epoch 120  Loss 1.7217\n",
            "Epoch 120  Loss 1.8755\n",
            "Epoch 120  Loss 2.2470\n",
            "Epoch 120  Loss 2.3939\n",
            "Epoch 120  Loss 2.5322\n",
            "Epoch 120  Loss 2.7433\n",
            "Epoch 120  Loss 3.0139\n",
            "Epoch 120  Loss 3.2166\n",
            "Epoch 120  Loss 3.4268\n",
            "Epoch 120  Loss 3.4990\n",
            "Epoch 120  Loss 3.5915\n",
            "Epoch 120  Loss 3.8368\n",
            "Epoch 120  Loss 3.9839\n",
            "Epoch 120  Loss 4.0888\n",
            "Epoch 120  Loss 4.3251\n",
            "Epoch 120  Loss 4.8469\n",
            "Epoch 120  Loss 5.1747\n",
            "Epoch 120  Loss 5.2439\n",
            "Epoch 120  Loss 5.2817\n",
            "Epoch 120  Loss 5.5164\n",
            "Epoch 120  Loss 5.6666\n",
            "Epoch 120  Loss 5.8833\n",
            "Epoch 120  Loss 6.0842\n",
            "Epoch 120  Loss 6.5458\n",
            "Epoch 120  Loss 6.6437\n",
            "Epoch 120  Loss 6.7353\n",
            "Epoch 120  Loss 6.8860\n",
            "Epoch 120  Loss 6.9646\n",
            "Epoch 120  Loss 7.2639\n",
            "Epoch 120  Loss 7.3630\n",
            "Epoch 120  Loss 7.5553\n",
            "Epoch 120  Loss 7.6608\n",
            "Epoch 120  Loss 7.6608\n",
            "Epoch 140  Loss 0.1138\n",
            "Epoch 140  Loss 0.2135\n",
            "Epoch 140  Loss 0.2759\n",
            "Epoch 140  Loss 0.4687\n",
            "Epoch 140  Loss 0.7309\n",
            "Epoch 140  Loss 0.8925\n",
            "Epoch 140  Loss 1.0505\n",
            "Epoch 140  Loss 1.1711\n",
            "Epoch 140  Loss 1.3020\n",
            "Epoch 140  Loss 1.3230\n",
            "Epoch 140  Loss 1.4454\n",
            "Epoch 140  Loss 1.5661\n",
            "Epoch 140  Loss 2.0985\n",
            "Epoch 140  Loss 2.1835\n",
            "Epoch 140  Loss 2.2757\n",
            "Epoch 140  Loss 2.3538\n",
            "Epoch 140  Loss 2.6480\n",
            "Epoch 140  Loss 2.9675\n",
            "Epoch 140  Loss 3.1893\n",
            "Epoch 140  Loss 3.2436\n",
            "Epoch 140  Loss 3.4501\n",
            "Epoch 140  Loss 3.7215\n",
            "Epoch 140  Loss 3.9527\n",
            "Epoch 140  Loss 4.1234\n",
            "Epoch 140  Loss 4.2042\n",
            "Epoch 140  Loss 4.4099\n",
            "Epoch 140  Loss 4.7349\n",
            "Epoch 140  Loss 4.9569\n",
            "Epoch 140  Loss 5.1537\n",
            "Epoch 140  Loss 5.2232\n",
            "Epoch 140  Loss 5.5146\n",
            "Epoch 140  Loss 5.6452\n",
            "Epoch 140  Loss 5.8741\n",
            "Epoch 140  Loss 6.5113\n",
            "Epoch 140  Loss 6.7348\n",
            "Epoch 140  Loss 6.8794\n",
            "Epoch 140  Loss 7.0186\n",
            "Epoch 140  Loss 7.5692\n",
            "Epoch 140  Loss 7.6479\n",
            "Epoch 140  Loss 7.6535\n",
            "Epoch 160  Loss 0.1786\n",
            "Epoch 160  Loss 0.2341\n",
            "Epoch 160  Loss 0.3496\n",
            "Epoch 160  Loss 0.6747\n",
            "Epoch 160  Loss 0.8229\n",
            "Epoch 160  Loss 0.9497\n",
            "Epoch 160  Loss 1.0714\n",
            "Epoch 160  Loss 1.1061\n",
            "Epoch 160  Loss 1.3093\n",
            "Epoch 160  Loss 1.5546\n",
            "Epoch 160  Loss 1.8177\n",
            "Epoch 160  Loss 1.9427\n",
            "Epoch 160  Loss 2.0711\n",
            "Epoch 160  Loss 2.3018\n",
            "Epoch 160  Loss 2.3775\n",
            "Epoch 160  Loss 2.5966\n",
            "Epoch 160  Loss 2.7354\n",
            "Epoch 160  Loss 2.8350\n",
            "Epoch 160  Loss 2.8827\n",
            "Epoch 160  Loss 2.9705\n",
            "Epoch 160  Loss 3.1624\n",
            "Epoch 160  Loss 3.3340\n",
            "Epoch 160  Loss 3.4689\n",
            "Epoch 160  Loss 3.5869\n",
            "Epoch 160  Loss 3.8778\n",
            "Epoch 160  Loss 4.1040\n",
            "Epoch 160  Loss 4.2742\n",
            "Epoch 160  Loss 4.3842\n",
            "Epoch 160  Loss 4.6694\n",
            "Epoch 160  Loss 5.1320\n",
            "Epoch 160  Loss 5.4210\n",
            "Epoch 160  Loss 5.7002\n",
            "Epoch 160  Loss 5.7499\n",
            "Epoch 160  Loss 6.0205\n",
            "Epoch 160  Loss 6.1744\n",
            "Epoch 160  Loss 7.0415\n",
            "Epoch 160  Loss 7.2261\n",
            "Epoch 160  Loss 7.4139\n",
            "Epoch 160  Loss 7.5406\n",
            "Epoch 160  Loss 7.6971\n",
            "Epoch 180  Loss 0.3543\n",
            "Epoch 180  Loss 0.4651\n",
            "Epoch 180  Loss 0.6668\n",
            "Epoch 180  Loss 0.6899\n",
            "Epoch 180  Loss 0.8726\n",
            "Epoch 180  Loss 0.9814\n",
            "Epoch 180  Loss 1.3592\n",
            "Epoch 180  Loss 1.4203\n",
            "Epoch 180  Loss 1.6995\n",
            "Epoch 180  Loss 1.9420\n",
            "Epoch 180  Loss 2.2614\n",
            "Epoch 180  Loss 2.4017\n",
            "Epoch 180  Loss 2.6785\n",
            "Epoch 180  Loss 2.8320\n",
            "Epoch 180  Loss 2.9558\n",
            "Epoch 180  Loss 3.0117\n",
            "Epoch 180  Loss 3.0745\n",
            "Epoch 180  Loss 3.3673\n",
            "Epoch 180  Loss 3.4757\n",
            "Epoch 180  Loss 3.7066\n",
            "Epoch 180  Loss 3.8040\n",
            "Epoch 180  Loss 3.9513\n",
            "Epoch 180  Loss 4.0388\n",
            "Epoch 180  Loss 4.0753\n",
            "Epoch 180  Loss 4.3225\n",
            "Epoch 180  Loss 4.4324\n",
            "Epoch 180  Loss 4.4887\n",
            "Epoch 180  Loss 4.8465\n",
            "Epoch 180  Loss 5.0644\n",
            "Epoch 180  Loss 5.1672\n",
            "Epoch 180  Loss 5.3734\n",
            "Epoch 180  Loss 5.9687\n",
            "Epoch 180  Loss 6.0384\n",
            "Epoch 180  Loss 6.1238\n",
            "Epoch 180  Loss 6.2898\n",
            "Epoch 180  Loss 6.3614\n",
            "Epoch 180  Loss 6.9934\n",
            "Epoch 180  Loss 7.3143\n",
            "Epoch 180  Loss 7.5106\n",
            "Epoch 180  Loss 7.6618\n"
          ]
        }
      ]
    }
  ]
}